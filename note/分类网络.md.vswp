vnote_backup_file_826537664 /home/mao/note/note/分类网络.md
# VGG
![Selection_340](_v_images/20200214164341156_490738365.png =512x)
Configurations of VGG; depth increases from left to right and the added layers are bolded. The convolutional layer parameters are denoted as “conv<receptive field size> — <number of channels>”.

网络结构简单,每一个conv layer由3x3conv + bn(可选) + relu组成,再不断的stack组成cnn,经典的是VGG-16和VGG-19
kernel size采用3x3,相比之前的5x5, 7x7, 11x11来说参数量和计算量都减少很多, 我们还知道2个3x3conv的cover area和一个5x5conv相当
其中有用到1x1conv,可以用于降低channel,同时参数量小,计算量也小. 还可以增强网络的非线性能力

# ResNet
![Selection_343](_v_images/20200214172403562_938207369.png =616x)
ResNet Architectures

主要还是skip connection,分为identity mapping(dimension一样),projection shortcut(dimension不一样), 用于解决网络层数越多而造成的梯度消失问题. identity mapping就是输入x+output, projection shortcut就是输入self.conv(x)+output
![Selection_341](_v_images/20200214172048312_1617541162.png)
BasicBlock

![Selection_342](_v_images/20200214172125077_833305647.png)
Bottleneck,输入和输出的channel大,中间的channel小

# MobileNetV2
![Selection_344](_v_images/20200214183847815_1987735144.png)
Comparison between the conventional residual layer and the inverted residual layer

![Selection_345](_v_images/20200214184035027_1308904142.png)

![Selection_346](_v_images/20200214184259150_700866507.png)
这里的t用于控制model thinner, 其实是控制output channel